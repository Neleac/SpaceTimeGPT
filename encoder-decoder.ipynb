{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaf3f10-c8ad-47fd-b95b-9245da9bb0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import VideoReader\n",
    "from transformers import VisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0840708-8e75-4bf3-a615-5eecfa8760f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k600 were not used when initializing TimesformerModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.q_attn.weight', 'h.6.ln_cross_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.5.crossattention.masked_bias', 'h.3.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.bias', 'h.0.ln_cross_attn.weight', 'h.9.crossattention.masked_bias', 'h.7.crossattention.c_proj.bias', 'h.2.crossattention.masked_bias', 'h.2.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.10.crossattention.bias', 'h.11.crossattention.masked_bias', 'h.10.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.0.crossattention.masked_bias', 'h.5.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.8.crossattention.bias', 'h.6.crossattention.bias', 'h.2.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.7.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.2.crossattention.bias', 'h.5.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.1.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.7.ln_cross_attn.weight', 'h.4.crossattention.masked_bias', 'h.1.crossattention.c_proj.bias', 'h.9.crossattention.bias', 'h.10.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.11.crossattention.bias', 'h.3.crossattention.masked_bias', 'h.1.crossattention.bias', 'h.4.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.5.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.10.crossattention.masked_bias', 'h.9.ln_cross_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.0.crossattention.bias', 'h.8.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.masked_bias', 'h.4.crossattention.bias', 'h.1.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.3.crossattention.bias', 'h.5.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.10.ln_cross_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.weight', 'h.7.crossattention.bias', 'h.11.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.6.crossattention.masked_bias', 'h.4.ln_cross_attn.weight', 'h.8.crossattention.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274065408\n"
     ]
    }
   ],
   "source": [
    "# TimeSformer\n",
    "encoder = \"facebook/timesformer-base-finetuned-k600\"\n",
    "decoder = \"gpt2\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder)\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder, decoder).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(model.num_parameters())\n",
    "\n",
    "# VideoMAE\n",
    "# encoder = \"MCG-NJU/videomae-base\"\n",
    "# decoder = \"gpt2\"\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(encoder)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(decoder)\n",
    "# model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5683db-27da-48e8-a203-da3b65acb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VatexDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, videos_path, json_path, num_frames, transforms=None):\n",
    "        self.video_names = []\n",
    "        for _, _, files in os.walk(videos_path):\n",
    "            for file in files:\n",
    "                self.video_names.append(file)\n",
    "        \n",
    "        self.video_captions = {}\n",
    "        with open(json_path) as json_file:\n",
    "            json_data = json.load(json_file)            \n",
    "            for data in json_data:\n",
    "                video_id = data[\"videoID\"]\n",
    "                captions = data[\"enCap\"]\n",
    "                self.video_captions[video_id] = captions\n",
    "                \n",
    "        self.videos_path = videos_path\n",
    "        self.num_frames = num_frames\n",
    "        self.transforms = transforms\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_names) * 10\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        video_idx = idx % len(self.video_names)\n",
    "        caption_idx = idx // len(self.video_names)\n",
    "        \n",
    "        video_name = self.video_names[video_idx]\n",
    "        video_id = video_name.split(\".\")[0]\n",
    "        captions = self.video_captions[video_id]\n",
    "        caption = captions[caption_idx]\n",
    "        \n",
    "        video_path = os.path.join(self.videos_path, video_name)\n",
    "        reader = VideoReader(video_path, \"video\")\n",
    "        \n",
    "        duration = reader.get_metadata()[\"video\"][\"duration\"][0]\n",
    "        step = duration / self.num_frames\n",
    "        \n",
    "        frames = []\n",
    "        for i in range(self.num_frames):\n",
    "            timestamp = random.uniform(i * step, (i + 1) * step)\n",
    "            reader.seek(timestamp)\n",
    "            frames.append(next(reader)[\"data\"])\n",
    "\n",
    "        return (frames, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd326d63-0c9d-4dad-9123-e288339455fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "num_frames = 8\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    T.RandomPerspective(),\n",
    "    T.RandomRotation(15),\n",
    "    T.RandomAdjustSharpness(2),\n",
    "    T.RandomAutocontrast(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(image_processor.image_mean, image_processor.image_std)\n",
    "])\n",
    "\n",
    "transforms_val = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(image_processor.image_mean, image_processor.image_std),\n",
    "])\n",
    "\n",
    "data_train = VatexDataset(\"dataset/vatex_train_videos\", \"dataset/vatex_train_captions.json\", num_frames, transforms_train)\n",
    "data_val = VatexDataset(\"dataset/vatex_val_videos\", \"dataset/vatex_val_captions.json\", num_frames, transforms_val)\n",
    "\n",
    "data_train = torch.utils.data.Subset(data_train, list(range(100)))\n",
    "data_val = torch.utils.data.Subset(data_val, list(range(10)))\n",
    "\n",
    "print(len(data_train))\n",
    "print(len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be0dcf2-f55b-401b-9897-08f5624902fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/922201615/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 5\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 5\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 274065408\n",
      "/home/922201615/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:147: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541990/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17/100 01:00 < 05:34, 0.25 it/s, Epoch 2.25/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.231000</td>\n",
       "      <td>0.073653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to trainer/checkpoint-5\n",
      "Configuration saved in trainer/checkpoint-5/config.json\n",
      "Configuration saved in trainer/checkpoint-5/generation_config.json\n",
      "Model weights saved in trainer/checkpoint-5/pytorch_model.bin\n",
      "Image processor saved in trainer/checkpoint-5/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to trainer/checkpoint-11\n",
      "Configuration saved in trainer/checkpoint-11/config.json\n",
      "Configuration saved in trainer/checkpoint-11/generation_config.json\n",
      "Model weights saved in trainer/checkpoint-11/pytorch_model.bin\n",
      "Image processor saved in trainer/checkpoint-11/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 5\n",
      "Saving model checkpoint to trainer/checkpoint-16\n",
      "Configuration saved in trainer/checkpoint-16/config.json\n",
      "Configuration saved in trainer/checkpoint-16/generation_config.json\n",
      "Model weights saved in trainer/checkpoint-16/pytorch_model.bin\n",
      "Image processor saved in trainer/checkpoint-16/preprocessor_config.json\n",
      "There seems to be not a single sample in your epoch_iterator, stopping training at step 16! This is expected if you're using an IterableDataset and set num_steps (100) higher than the number of available samples.\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     19\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m     20\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     tf32\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     32\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     33\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mimage_processor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py:1883\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1883\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1886\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1887\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/trainer.py:2112\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;66;03m# reset tr_loss to zero\u001b[39;00m\n\u001b[1;32m   2110\u001b[0m tr_loss \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss\n\u001b[0;32m-> 2112\u001b[0m logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[43mtr_loss_scalar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_globalstep_last_logged\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m   2113\u001b[0m logs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_learning_rate()\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_loss_scalar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss_scalar\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values, labels = [], []\n",
    "    for frames, caption in examples:\n",
    "        pixel_values.append(image_processor(frames, return_tensors=\"pt\").pixel_values)\n",
    "        labels.append(torch.LongTensor(tokenizer(caption, padding=\"max_length\").input_ids))\n",
    "    \n",
    "    pixel_values = torch.cat(pixel_values)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "metric = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"trainer\",\n",
    "    tf32=True,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=5,\n",
    "    per_device_eval_batch_size=5,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=image_processor,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5dc5c-5959-40cb-b680-468a19a13eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames, caption = data_train[11]\n",
    "# print(caption)\n",
    "\n",
    "# pixel_values = image_processor(frames, return_tensors=\"pt\").pixel_values.to(device)\n",
    "# generated_ids = model.generate(pixel_values)\n",
    "# generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8992b8-78cd-443b-ab93-a564c7555e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
