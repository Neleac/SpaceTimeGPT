{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaf3f10-c8ad-47fd-b95b-9245da9bb0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "GPUs:\n",
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES 0\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datasets import load_from_disk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda:0\":\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"GPUs:\")\n",
    "    for i in range(n_gpus):\n",
    "        print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0840708-8e75-4bf3-a615-5eecfa8760f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at facebook/timesformer-base-finetuned-k600 were not used when initializing TimesformerModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TimesformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TimesformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.2.ln_cross_attn.weight', 'h.11.crossattention.masked_bias', 'h.4.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.1.crossattention.masked_bias', 'h.0.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.4.crossattention.masked_bias', 'h.2.crossattention.c_proj.bias', 'h.4.crossattention.bias', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.2.crossattention.bias', 'h.8.crossattention.masked_bias', 'h.6.crossattention.masked_bias', 'h.3.crossattention.masked_bias', 'h.3.crossattention.bias', 'h.0.crossattention.bias', 'h.2.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.5.crossattention.bias', 'h.5.crossattention.masked_bias', 'h.4.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.5.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.7.crossattention.bias', 'h.3.ln_cross_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.9.crossattention.masked_bias', 'h.1.crossattention.bias', 'h.4.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.7.crossattention.masked_bias', 'h.4.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.11.ln_cross_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.6.crossattention.bias', 'h.0.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.10.crossattention.c_attn.weight', 'h.0.crossattention.masked_bias', 'h.2.crossattention.masked_bias', 'h.5.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.weight', 'h.10.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.11.crossattention.bias', 'h.6.crossattention.c_proj.weight', 'h.7.ln_cross_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.8.crossattention.bias', 'h.9.crossattention.bias', 'h.4.crossattention.q_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# TimeSformer\n",
    "encoder = \"facebook/timesformer-base-finetuned-k600\"\n",
    "decoder = \"gpt2\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder, decoder).to(device)\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.max_length = 50\n",
    "model.config.num_beams = 4\n",
    "model.config.early_stopping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e751eba-19bf-4ea4-844e-1badd1ff9a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['video_id', 'pixel_values', 'labels'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['video_id', 'pixel_values', 'labels'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"dataset/processed/k600_16frames\")\n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].select(np.arange(6))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(np.arange(6))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3add6c94-649d-4a67-92fb-bc02bccf3609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/922201615/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/922201615/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/922201615/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "The following columns in the training set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: video_id. If video_id are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/home/922201615/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 6\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 274065408\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.052400</td>\n",
       "      <td>7.364590</td>\n",
       "      <td>0.010654</td>\n",
       "      <td>0.048474</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: video_id. If video_id are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 6\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"early_stopping\": true,\n",
      "  \"max_length\": 50,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "/home/922201615/anaconda3/envs/vlm/lib/python3.10/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A girl staring at the camera while she takes her hand and places her index finger on her nose', 'A woman is pressing her nose with her finger and then she smiles a few times.', 'A young woman stares into the camera smiling and pressing down her nose with one finger.', 'A girl is smiling and playing with her nose as music plays in the background.', 'A young girl smiles and presses the end of her nose with her index finger.', 'A girl holds her finger to her nose then puts it down as she smiles.', 'A girl listening to music and touching her nose and smiling.', 'Girl listens to music, show reaction, covers nostril, smile, uncovers nostril', 'A selfie video of young person sitting and listening to music.', 'A young asian girl is making faces into her phone.']\n",
      "['A person with a faint moustache and thick black hair is fiddling with the quiff of his hair at the front while staring into a camera.', 'A young man is taking a selfie video of him playing with his black hair.', 'A man plays with his hair as a Chain Smoker song plays in the background.', 'A person stroke down some stands of his hair down the side of his head', 'A young boy is fixing his hair with his fingers as it is sticking up.', 'A young man is styling his hair while some music is playing in the background.', 'A young man plays with his hair, flattening his bangs with his fingers.', 'A young man styles a curl of his hair near his right eye.', 'A man is fixing his hair with his hand inside a room.', 'A selfie shot of a feminine man playing with his hair.']\n",
      "[\"A baby is laying on it's back staring at someone until it finally sticks it's tongue out.\", 'A baby feels happy making different movements with his body and gestures with his face.', \"A baby moves it's arms around and looks concerned as at stares at the camera\", 'A baby lies on a bed and smiles as his mother talks to him.', 'An infant, lying on his back, moves his arms and sticks his tongue out.', 'A little boy is lying on a bed and showing his tongue', 'A baby wiggles around and makes faces while laying on its back.', 'A baby smiles as it dances and stares at the camera.', 'A baby is laying on his back and making faces.', 'A baby is being filmed making facial expressions.']\n",
      "['As 2 women are talking  and laughing off camera, the newborn infant is bundled up and is mean mugging the camera.', 'A baby is laying in a hospital bassinet just staring, a woman comes and pulls another baby in a bassinet in.', 'A newborn baby is in a baby bed in a hospital and it is swaddled tightly in a blanket.', 'A small baby wrapped in coddling is resting in a sterile hospital environment as adults chatter.', 'A baby is wrapped with a towel and is laying on top of a bed.', 'A newborn swaddled in a crib at a hospital and nurses talking in the background.', 'A baby is laying in its nursery bed wrapped in a blanket.', 'The baby girl is looking curiously at the woman speaking to her.', 'A baby is laying down as several people laugh off camera.', 'A baby is laying while women talk in the background.']\n",
      "['A person is trying to ride a bike down a small snow covered hill while someone is laughing in the voice over.', 'A guy is trying to ride his bike through the thick snow and having a hard time doing it.', 'Someone is trying to ride a bicycle down a snow covered hill while the person recording is laughing.', 'a young boy is really struggling to move his bike through the snow for some yards', 'Someone tries to ride a dirt bike in deep powder snow as a girl laughs.', 'A person is trying to ride their bike down a hill in the snow.', 'Someone slowly rides a bike down a snowy hill as an unseen person laughs.', 'A boy rides his bike in the snow, but does not get very far.', 'A person is trying to ride a bike through snow down an ambankment.', 'A person on a mountain bike rides through the snow']\n",
      "['A child is jumping on a bouny castle with an adult female, she is laughing and jumbing and an unseen male is speaking to her.', 'A young girl is jumping up and down inside of a bounce house with an adult woman.', 'A little girl is learning to jump in a bouncy house with the aid of a woman.', 'A little girl in a pink coat is jumping in a bounce house with her relative.', 'A young kid is happily jumping on a trampoline and accompanied by an adult.', 'A young little girl jumping in a bouncy house with an older woman.', 'a young girl is stumbling around in a bouncy house and falling', 'A little girl and an adult playing in an inflateable bounce castle.', 'A little girl is jumping up and down in an inflatable.', 'A young girl jumps with an adult in a bouncy house.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to training/random_frames/checkpoint-1\n",
      "Configuration saved in training/random_frames/checkpoint-1/config.json\n",
      "Configuration saved in training/random_frames/checkpoint-1/generation_config.json\n",
      "Model weights saved in training/random_frames/checkpoint-1/pytorch_model.bin\n",
      "tokenizer config file saved in training/random_frames/checkpoint-1/tokenizer_config.json\n",
      "Special tokens file saved in training/random_frames/checkpoint-1/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from training/random_frames/checkpoint-1 (score: 7.364589691162109).\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"training/random_frames\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    tf32=True,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    dataloader_num_workers=8,\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=5e-7,\n",
    ")\n",
    "\n",
    "def collator(examples):\n",
    "    pixel_values, labels = [], []\n",
    "    for example in examples:\n",
    "        # train\n",
    "        if len(example[\"pixel_values\"]) == 16:\n",
    "            frame_idxs = []\n",
    "            for i in range(0, 16, 2):\n",
    "                frame_idxs.append(i + random.randint(0, 1))\n",
    "            pixel_values.append(example[\"pixel_values\"][frame_idxs])\n",
    "        # val\n",
    "        else:\n",
    "            pixel_values.append(example[\"pixel_values\"])\n",
    "        labels.append(example[\"labels\"])\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    labels = torch.stack(labels)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "val_output = {}\n",
    "with open(\"dataset/longestCaption_videoID.json\") as file:\n",
    "    longestCaption_videoID = json.load(file)\n",
    "with open(\"dataset/videoID_captions.json\") as file:\n",
    "    videoID_captions = json.load(file)\n",
    "    \n",
    "def metrics(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    references = []\n",
    "    for i, label in enumerate(labels):\n",
    "        video_id = longestCaption_videoID[label]\n",
    "        references.append(videoID_captions[video_id])\n",
    "        if video_id in val_output:\n",
    "            val_output[video_id].append(predictions[i])\n",
    "        else:\n",
    "            val_output[video_id] = [predictions[i]]\n",
    "            \n",
    "    for reference in references:\n",
    "        print(reference)\n",
    "            \n",
    "    bleu_scores = bleu.compute(predictions=predictions, references=references, smooth=True)\n",
    "    meteor_scores = meteor.compute(predictions=predictions, references=references)\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references, rouge_types=['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)    \n",
    "    return {\"bleu\": bleu_scores[\"bleu\"], \"meteor\": meteor_scores[\"meteor\"], \"rouge1\": rouge_scores[\"rouge1\"], \"rouge2\": rouge_scores[\"rouge2\"], \"rougeL\": rouge_scores[\"rougeL\"]}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    compute_metrics=metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "with open(os.path.join(output_dir, \"val_output.json\"), \"w\") as file:\n",
    "    file.write(json.dumps(val_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
