{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07fef3dd-ca49-42f0-8a5b-8379a040dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from datasets.combine import concatenate_datasets\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "\n",
    "FRAMES_PER_VIDEO = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3a3afe7-d7ff-4ebd-a4a8-6701cb46f47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7145d06b-b928-4661-8034-7751ecd4e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    video_id = example[\"videoID\"]\n",
    "    captions = example[\"enCap\"]\n",
    "    \n",
    "    videos_path = \"dataset/videos\"\n",
    "    video_path = os.path.join(videos_path, \"%s.mp4\" % video_id)\n",
    "    if not os.path.isfile(video_path):\n",
    "        video_path = os.path.join(videos_path, \"%s.webm\" % video_id)\n",
    "    \n",
    "    # count number of frames\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, _ = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "    video.release()\n",
    "        \n",
    "    # fixed frame sampling\n",
    "    indices = np.linspace(0, frame_count, num=FRAMES_PER_VIDEO, endpoint=False).astype(np.int64)\n",
    "    # random frame sampling\n",
    "    #indices = np.sort(np.random.uniform(low=0, high=frame_count, size=self.num_frames).astype(np.int64))\n",
    "    \n",
    "    # get frames\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count, frame_idx = 0, 0\n",
    "    while frame_idx < len(indices):\n",
    "        if frame_count == indices[frame_idx]:\n",
    "            _, frame = video.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "            frame_idx += 1\n",
    "        else:\n",
    "            video.grab()\n",
    "        frame_count += 1\n",
    "    video.release()\n",
    "        \n",
    "    # longest caption\n",
    "    max_len = -np.inf\n",
    "    caption = None\n",
    "    for cap in captions:\n",
    "        length = len(cap.split(\" \"))\n",
    "        if length > max_len:\n",
    "            max_len = length\n",
    "            caption = cap\n",
    "    # random caption\n",
    "    #caption = captions[random.randint(0, 9)]\n",
    "    \n",
    "    pixel_values = image_processor(frames, return_tensors=\"pt\").pixel_values\n",
    "    labels = tokenizer(caption, padding=\"max_length\").input_ids\n",
    "    return {\"pixel_values\": pixel_values[0], \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364cc2f7-5c8b-432d-9fc4-4bb8e0d9097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8e787070c0da13e4\n",
      "Found cached dataset json (/home/922201615/.cache/huggingface/datasets/json/default-8e787070c0da13e4/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91dba0c81ebd48dea4833045620a3f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load json data\n",
    "\n",
    "data_files = {\"train\": \"dataset/vatex_train_captions.json\", \"validation\": \"dataset/vatex_val_captions.json\"}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbed07-906b-40f2-804b-df046343754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and save parts\n",
    "\n",
    "parts_dir = \"dataset/parts\"\n",
    "step = 1000\n",
    "\n",
    "start, end = 0, len(dataset[\"train\"])\n",
    "idx = 0\n",
    "\n",
    "while start < end:\n",
    "    ds = DatasetDict({\"train\": dataset[\"train\"].select([x for x in range(start, min(start + step, end))])})\n",
    "    ds = ds.map(function=preprocess, remove_columns=ds[\"train\"].column_names)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    ds.save_to_disk(os.path.join(parts_dir, \"part%d\" % idx))\n",
    "    \n",
    "    start += step\n",
    "    idx += 1\n",
    "    \n",
    "start, end = 0, len(dataset[\"validation\"])\n",
    "\n",
    "while start < end:\n",
    "    ds = DatasetDict({\"validation\": dataset[\"validation\"].select([x for x in range(start, min(start + step, end))])})\n",
    "    ds = ds.map(function=preprocess, remove_columns=ds[\"validation\"].column_names)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    ds.save_to_disk(os.path.join(parts_dir, \"part%d\" % idx))\n",
    "    \n",
    "    start += step\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "325163cf-0126-43be-948b-5c84a10e4354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['pixel_values', 'labels'],\n",
       "        num_rows: 22895\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['pixel_values', 'labels'],\n",
       "        num_rows: 2643\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge parts and save combined\n",
    "\n",
    "dataset = DatasetDict({\"train\": None, \"validation\": None})\n",
    "\n",
    "for part in os.listdir(parts_dir):\n",
    "    ds = load_from_disk(os.path.join(parts_dir, part))\n",
    "    \n",
    "    for split in (\"train\", \"validation\"):\n",
    "        if split in ds:\n",
    "            if dataset[split] is None:\n",
    "                dataset[split] = ds[split]\n",
    "            else:\n",
    "                dataset[split] = concatenate_datasets([dataset[split], ds[split]])\n",
    "\n",
    "dataset.save_to_disk(\"dataset/preprocessed\")\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
